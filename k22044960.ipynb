{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32b2f230",
   "metadata": {},
   "source": [
    "# MLE 24/25 - Coursework 1\n",
    "\n",
    "## Guidelines\n",
    "\n",
    "### Setting up the coursework\n",
    "\n",
    "To start, download the file `cw2.zip` from the moduleâ€™s Keats website. Once this is done:\n",
    "- Unzip the file `cw2.zip` in a folder of your choice. We will refer to this folder as `<FOLDER>`.\n",
    "- Make a duplicate of the Notebook file `<FOLDER>/coursework.ipynb` and rename it to `<FOLDER>/<K_NUM>.ipynb`. For instance, if your k number is \"k12345678\", your coursework file will be located at `<FOLDER>/k12345678.ipynb`.\n",
    "- Open Anaconda, select the environment \"ml4eng\", and launch Jupyter Lab. Open your coursework file `<FOLDER>/<K_NUM>.ipynb`.\n",
    " \n",
    "### Instructions\n",
    "\n",
    "In this coursework, you will complete a series of functions that will be called in the following cells.\n",
    "Each question will specify which function should be completed, as well as its expected behaviour.\n",
    "**You must ONLY modify the code inside the mentioned functions at each question. Most importantly, you must NOT edit the name of the provided functions, or the order/name of its input parameters.**\n",
    "**If you do, we will be unable to mark your project.**\n",
    "We will also specify the format of the inputs and the required format of the outputs in the function description.\n",
    "\n",
    "For instance, let's assume we have a question that asks us to complete function named `example`, which sums the entries of a vector `v` given as input.\n",
    "The function to complete will be presented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9abaa725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example(v):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    v : np.ndarray\n",
    "        N x 1 numpy array with `float` elements\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sum_v : float\n",
    "        Sum of elements in v\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93763b0",
   "metadata": {},
   "source": [
    "_Note: we use `np.ndarray` to indicate the corresponding variable is a NumPy array._\n",
    "\n",
    "**Functions that do not respect the specified output format or order will not be awarded any point.**\\\n",
    "**Additionally, your function must only rely on the given inputs or the variables created within the function itself, and not use any variable created inside the Notebook instance.**\\\n",
    "Nonetheless, you are allowed to re-use functions implemented in previous questions.\n",
    "\n",
    "Please avoid defining two functions with the same name, write your code directly inside the available function template, i.e., just below the comment `# Write your code here`.\n",
    "\n",
    "Once a function has been completed, it will be usually called in the following cells, and you will be able to check that it provides the intended results.\n",
    "If you want to inspect your function in more detail, we recommend that you do so in a separate Notebook (e.g., by duplicating the present Notebook) and keep the file `<K_NUM>.ipynb` for your final implementation.\n",
    "\n",
    "Please avoid submitting functions that display text in your final submission.\n",
    "You are free (and encouraged) to add `print` statements while completing the functions, but do not forget to remove them in the final submission.\n",
    "\n",
    "You are only allowed to use the libraries loaded in the section \"Import libraries\", that is: `numpy`, `scipy.stats` and `matplotlib.pyplot`.\n",
    "\n",
    "\n",
    "### Submitting your coursework\n",
    "\n",
    "Before submitting your work, you must re-run your completed Notebook with a clean Notebook instance in order to ensure that there are no variables defined in your instance that are causing side-effects.\n",
    "For this, follow the instructions below:\n",
    "- Restart your kernel and clear your outputs (in the menu bar of Jupyter Lab: `Kernel > Restart Kernel and Clear Outputs of All Cells...`): this will remove all pre-existing variables and outputs of your code cells.\n",
    "- Re-run all cells of your notebook in order (_hint: you can use `shift+enter` to run a cell and select the next cell_)\n",
    "\n",
    "Please make sure that all functions output the right dimensions, as specified in the corresponding question, and that no function raises an error.\n",
    "**No points will be awarded for functions that do not output the right dimensions, or to functions that raise an error.**\n",
    "\n",
    "Finally, compress your Notebook file `<K_NUM>.ipynb` into a zip file with name `<K_NUM>.zip` and submit it to KEATS.\n",
    "**Please ensure your `.zip` file ONLY contains the notebook you wish to submit. You do not have to include the dataset in your `.zip` file and you MUST NOT submit multiple Notebooks, only include the one you wish to be marked.**\n",
    "\n",
    "**Again, please ensure all cells of the submitted Notebook are executed, even if the corresponding cell results in an error.**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this coursework, we will work with a modified version of the _Diabetes Dataset_ from the National Institute of Diabetes and Digestive and Kidney Diseases.\n",
    "This dataset includes $N^\\mathrm{all} = 768$ entries, each representing $K = 7$ clinical features of a given patient plus a binary target variable, which indicates whether or not the patient developed diabetes.\n",
    "The goal of this coursework will be to learn a predictor that quantifies the likelihood of a partient developping diabetes based on their clinical features.\n",
    "\n",
    "A description of the features and target variables is available in the table below, along with the column index of each variable.\n",
    "\n",
    "\n",
    "| Name                       | Column Index | Feature/Target | Max Value | Description                                                                                                                              |\n",
    "|----------------------------|--------------| -------------- | --------- | ---------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| Age                        | 0            | Feature        | 81        | The age of the patient in years.                                                                                                         |\n",
    "| BloodPressure              | 1            | Feature        | 122       | The diastolic blood pressure ($\\text{mm} \\cdot \\text{Hg}$) of the patient.                                                               |\n",
    "| BMI                        | 2            | Feature        | 67.1      | Body Mass Index, calculated as weight in $\\text{kg} \\cdot \\text{m}^{-2}$.                                                                |\n",
    "| Diabetes Pedigree Function | 3            | Feature        | 2.42      | A function that scores the likelihood of diabetes based on family history. Higher values indicate a stronger family history of diabetes. |\n",
    "| Glucose                    | 4            | Feature        | 199       | The plasma glucose concentration measured after a 2-hour oral glucose tolerance test ($\\text{mg} \\cdot \\text{dL}^{-1}$).                 |\n",
    "| Insulin                    | 5            | Feature        | 846       | The 2-hour serum insulin level ($\\mu \\text{U} \\cdot \\text{ml}^{-1}$).                                                                    |\n",
    "| Skin Thickness             | 6            | Feature        | 99        | The triceps skin fold thickness ($\\text{mm}$), which is a measure of body fat.                                                           |\n",
    "| Has Diabetes               | 7            | Target         | -         | Label equal to `1` if the patient has diabetes, and `0` if he/she does not.                                                              |\n",
    "\n",
    "\n",
    "The values of the features in the dataset have been normalized by their respective maximum value.\n",
    "Accordingly, all the values are in the range $[0, 1]$, where $1$ indicates that the feature is equal to its maximum value.\n",
    "\n",
    "We will denote the available dataset as $\\mathcal{D}^\\mathrm{all} = {(x_i, t_i)}_{i=0}^{N^\\mathrm{all}-1}$, where $N = 768$ is the number of available datapoints, $x_i \\in \\mathbb{R}^K$ is a $K \\times 1$ vector containing the $K = 7$ normalized feature values of the $i$-th datapoint (column indices from `0` to `6` in the table above), and $t_i \\in \\{0, 1\\}$ is its associated label (column index `7` above).\n",
    "We will use the notation\n",
    "\\begin{equation*}\n",
    "X_{\\mathcal{D}^\\mathrm{all}} = \\begin{bmatrix}\n",
    "(x_0)^\\top \\\\\n",
    "\\vdots \\\\\n",
    "(x_{N^\\mathrm{all}-1})^\\top\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "to denote the $N^\\mathrm{all} \\times K$ data matrix associated to the dataset $\\mathcal{D}^\\mathrm{all}$, and we will use\n",
    "\\begin{equation*}\n",
    "t_{\\mathcal{D}^\\mathrm{all}} = \\begin{bmatrix}\n",
    "t_0 \\\\\n",
    "\\vdots \\\\\n",
    "t_{N^\\mathrm{all}-1}\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "to denote the associated $N \\times 1$ targets vector.\n",
    "\n",
    "Throughout this coursework, we will use $\\mathcal{D} \\subset \\mathcal{D}^\\mathrm{all}$ to denote an arbitrary dataset subset of size $N \\leq N^\\mathrm{all}$. \n",
    "For simplicity, we will assume the elements of $\\mathcal{D} = {(x_i, t_i)}_{i=0}^{N-1}$ to be indexed from $0$ to $N-1$.\n",
    "In a similar fashion as above, we also define its associated $N \\times K$ data matrix $X_{\\mathcal{D}}$ and $N \\times 1$ target vector $t_{\\mathcal{D}}$.\n",
    "\n",
    "This coursework is split into two parts:\n",
    "- In the first part, we will train and evaluate a logistic regression algorithm.\n",
    "- In the second part, we will train and evaluate a neural network via backpropagation.\n",
    "\n",
    "\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e07a4722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e430ea21",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Loads the _Diabetes Dataset_ and splits it into $N^\\mathrm{tr} = 668$ training samples $\\mathcal{D}^{\\mathrm{tr}} = {(x_i, t_i)}_{i=0}^{N^\\mathrm{tr} - 1}$ and $N^\\mathrm{te} = N^\\mathrm{all} - N^\\mathrm{tr} = 100$ test samples $\\mathcal{D}^\\mathrm{te} = {(x_i, t_i)}_{i=N^\\mathrm{tr}}^{N-1}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cec16d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and split into test and training\n",
    "dataset_tr_te = np.load(\"dataset.npy\")\n",
    "dataset_tr = dataset_tr_te[:668, :]  # Training dataset\n",
    "dataset_te = dataset_tr_te[668:, :]  # Test dataset\n",
    "\n",
    "# Split dataset into inputs (covariates) and targets (labels)\n",
    "n_dataset_features = 7  # Number of covariate features in the dataset\n",
    "dataset_inputs_tr = dataset_tr[:, :n_dataset_features]  # Training data matrix\n",
    "dataset_targets_tr = dataset_tr[:, n_dataset_features].reshape(-1, 1)  # Training targets vector\n",
    "dataset_inputs_te = dataset_te[:, :n_dataset_features]  # Test data matrix\n",
    "dataset_targets_te = dataset_te[:, n_dataset_features].reshape(-1, 1)  # Test targets vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7e3909",
   "metadata": {},
   "source": [
    "## Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca2645c",
   "metadata": {},
   "source": [
    "In this first section, we will implement a logistic regression classifier.\n",
    "Accordingly, we will denote as $\\theta$ its $K \\times 1$ parameter vector, and a soft prediction will be given as $p(\\mathrm{t}_i = 1 | x_i, \\theta) = \\sigma(\\theta^\\top x_i)$, where\n",
    "\\begin{equation*}\n",
    "    \\sigma(x) = \\frac{1}{1 + \\exp(-x)}\n",
    "\\end{equation*}\n",
    "represents the sigmoid function applied to a real number $x \\in \\mathbb{R}$.\n",
    "\n",
    "### Question 1 [5 points]\n",
    "\n",
    "Complete the function `sigmoid` which takes as input a matrix \n",
    "\\begin{equation*}\n",
    "U = \\begin{bmatrix}\n",
    "u_{0, 0} & \\dots & u_{0, N_2 - 1} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "u_{N_1 - 1, 0} & \\dots & u_{N_1 - 1, N_2 - 1}\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "of any shape $N_1 \\times N_2$, and returns a matrix of the same shape\n",
    "\\begin{equation*}\n",
    "\\sigma(U) = \\begin{bmatrix}\n",
    "\\sigma(u_{0, 0}) & \\dots & \\sigma(u_{0, N_2 - 1}) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\sigma(u_{N_1 - 1, 0}) & \\dots & \\sigma(u_{N_1 - 1, N_2 - 1})\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "corresponding to the element-wise application of the sigmoid function to the coefficients of $U$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0477c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(mat):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    mat : np.ndarray\n",
    "        Matrix of arbitrary shape N_1 x N_2\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sigma_mat : np.ndarray\n",
    "        Matrix of shape N_1 x N_2 corresponding to the application of sigmoid function to the coefficients of `mat`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Write your code here\n",
    "\n",
    "    sigma_mat = 1 / (1 + np.exp(-mat))\n",
    "    \n",
    "    return sigma_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61545497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy matrix 'U':\n",
      "[[-3.         -2.79310345 -2.5862069  -2.37931034 -2.17241379 -1.96551724]\n",
      " [-1.75862069 -1.55172414 -1.34482759 -1.13793103 -0.93103448 -0.72413793]\n",
      " [-0.51724138 -0.31034483 -0.10344828  0.10344828  0.31034483  0.51724138]\n",
      " [ 0.72413793  0.93103448  1.13793103  1.34482759  1.55172414  1.75862069]\n",
      " [ 1.96551724  2.17241379  2.37931034  2.5862069   2.79310345  3.        ]]\n",
      "\n",
      "Sigmoid values 'sigma(U)':\n",
      "[[0.04742587 0.05769799 0.07003141 0.08476405 0.10225524 0.12287119]\n",
      " [0.14696317 0.17483739 0.20671728 0.24270043 0.28271489 0.32648243]\n",
      " [0.37349752 0.42303057 0.47416097 0.52583903 0.57696943 0.62650248]\n",
      " [0.67351757 0.71728511 0.75729957 0.79328272 0.82516261 0.85303683]\n",
      " [0.87712881 0.89774476 0.91523595 0.92996859 0.94230201 0.95257413]]\n"
     ]
    }
   ],
   "source": [
    "U = np.linspace(-3, 3, 30).reshape(5, 6)\n",
    "sigmoid_U = sigmoid(U)\n",
    "\n",
    "print(f\"Dummy matrix 'U':\\n{U}\\n\")\n",
    "print(f\"Sigmoid values 'sigma(U)':\\n{sigmoid_U}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085fd215",
   "metadata": {},
   "source": [
    "### Question 2 [10 points]\n",
    "\n",
    "Complete the function `logistic_regression_prediction` which takes as input a data matrix $X_{\\mathcal{D}}$ and a parameter vector $\\theta$, and returns the $N \\times 1$ vector of element-wise soft predictions $\\hat{p} = [p(\\mathrm{t}_0 = 1 | x_0, \\theta), ..., p(\\mathrm{t}_{N-1} = 1 | x_{N-1}, \\theta)]^\\top$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62f10f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_prediction(inputs, theta):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs : np.ndarray\n",
    "        N x K data matrix\n",
    "    \n",
    "    theta : np.ndarray\n",
    "        K x 1 parameter vector\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    p_hat : np.ndarray\n",
    "        N x 1 vector of element-wise soft predictions\n",
    "    \"\"\"\n",
    "\n",
    "    # Write your code here\n",
    "\n",
    "    # Compute logits for all data samples\n",
    "    logit = inputs @ theta \n",
    "\n",
    "    # Use sigmoid function to logits to get probabilities\n",
    "    p_hat = sigmoid(logit)\n",
    "\n",
    "    \n",
    "    return p_hat\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80f99cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of training inputs matrix:\n",
      "[[0.37  0.508 0.443 0.157 0.588 0.    0.121]\n",
      " [0.321 0.459 0.31  0.14  0.482 0.058 0.172]\n",
      " [0.42  0.525 0.455 0.579 0.533 0.141 0.354]\n",
      " [0.568 0.623 0.675 0.08  0.337 0.    0.   ]\n",
      " [0.296 0.492 0.529 0.171 0.543 0.21  0.465]]\n",
      "First 5 rows of training soft predictions (dummy parameters):\n",
      "[[0.36331611]\n",
      " [0.36262244]\n",
      " [0.27388502]\n",
      " [0.55057644]\n",
      " [0.3461513 ]]\n"
     ]
    }
   ],
   "source": [
    "dummy_theta = np.array([1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0]).reshape(-1, 1)\n",
    "dummy_logistic_reg_preds = logistic_regression_prediction(np.copy(dataset_inputs_tr), dummy_theta)\n",
    "\n",
    "print(\"First 5 rows of training inputs matrix:\")\n",
    "print(dataset_inputs_tr[:5, :])\n",
    "print(\"First 5 rows of training soft predictions (dummy parameters):\")\n",
    "print(dummy_logistic_reg_preds[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad1f880",
   "metadata": {},
   "source": [
    "### Question 3 [5 points]\n",
    "\n",
    "Complete the function `cross_entropy_loss` which takes as input the soft predictions $\\hat{p} = [p(\\mathrm{t}_0 = 1 | x_0, \\theta), ..., p(\\mathrm{t}_{N-1} = 1 | x_{N-1}, \\theta)]^\\top$ and the corresponding target vector $t_{\\mathcal{D}}$; and returns the cross-entropy loss\n",
    "\\begin{equation*}\n",
    "    \\mathcal{L}_{\\mathcal{D}}(\\theta) = \\frac{1}{N} \\sum_{i=0}^{N-1} \\ell_i(\\theta),\n",
    "\\end{equation*}\n",
    "under the parameters $\\theta$ on dataset $\\mathcal{D}$, where\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\ell_i(\\theta) &=  -\\log\\left( p(\\mathrm{t}_i = t_i | x_i, \\theta) \\right) \\\\\n",
    "    &= -t_i \\log\\left( p(\\mathrm{t}_i = 1 | x_i, \\theta) \\right) - (1 - t_i) \\log\\left( 1 - p(\\mathrm{t}_i = 1 | x_i, \\theta) \\right),\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "is the log-loss of the $i$-th datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6df4e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(soft_predictions, targets):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    soft_predictions : np.ndarray\n",
    "        N x 1 vector of element-wise soft predictions\n",
    "    \n",
    "    targets : np.ndarray\n",
    "        N x 1 target vector\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "        Scalar cross-entropy loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Write your code here\n",
    "\n",
    "    # Compute element-wise loss, and mean to find average loss\n",
    "    \n",
    "    log_loss = - np.mean(\n",
    "        targets * np.log(soft_predictions) + (1 - targets) * np.log(1 - soft_predictions)\n",
    "    )\n",
    "    return log_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd96c5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy targets:\n",
      "[[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Dummy soft predictions\n",
      "[[0.9]\n",
      " [0.5]\n",
      " [0.1]\n",
      " [0.3]\n",
      " [0.7]]\n",
      "Empirical cross-entropy loss: 0.662362764105494\n"
     ]
    }
   ],
   "source": [
    "dummy_targets = np.array([1., 0., 0., 1., 0.]).reshape(-1, 1)\n",
    "dummy_soft_preds = np.array([0.9, 0.5, 0.1, 0.3, 0.7]).reshape(-1, 1)\n",
    "dummy_loss = cross_entropy_loss(dummy_soft_preds, dummy_targets)\n",
    "\n",
    "print(\"Dummy targets:\")\n",
    "print(dummy_targets)\n",
    "print(\"Dummy soft predictions\")\n",
    "print(dummy_soft_preds)\n",
    "print(f\"Empirical cross-entropy loss: {dummy_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824dac20",
   "metadata": {},
   "source": [
    "### Question 4 [10 points]\n",
    "\n",
    "Complete the function `logistic_regression_gradient` which takes as input a data matrix $X_{\\mathcal{D}}$, its corresponding target vector $t_\\mathcal{D}$, and the soft predictions $\\hat{p} = [p(\\mathrm{t}_0 = 1 | x_0, \\theta), ..., p(\\mathrm{t}_{N-1} = 1 | x_{N-1}, \\theta)]^\\top$; and returns the gradient of the loss with respect to the parameters $\\theta$ as\n",
    "\\begin{equation*}\n",
    "    \\nabla \\mathcal{L}_{\\mathcal{D}}(\\theta) = \\frac{1}{N} \\sum_{i=0}^{N-1} \\nabla \\ell_i(\\theta),\n",
    "\\end{equation*}\n",
    "where $\\nabla \\mathcal{L}_{\\mathcal{D}}(\\theta)$ denotes the $K \\times 1$ gradient of $\\mathcal{L}_{\\mathcal{D}}(\\theta)$ with respect to the parameter vector $\\theta$.\n",
    "The gradient must be computed using **symbolic differentiation** (and **not** using numerical differentiation).\n",
    "The gradient is assumed to be evaluated at the same parameter vector $\\theta$ used to generate the soft prediction $\\hat{p}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75aca26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient(inputs, targets, soft_predictions):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs : np.ndarray\n",
    "        N x K data matrix\n",
    "    \n",
    "    targets : np.ndarray\n",
    "        N x 1 target vector\n",
    "    \n",
    "    soft_predictions : np.ndarray\n",
    "        N x 1 vector of element-wise soft predictions\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    gradient : np.ndarray\n",
    "        K x 1 gradient vector\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute mean error\n",
    "    mean_error = soft_predictions - targets\n",
    "\n",
    "    # Compute gradient using matrix multiplication\n",
    "    gradient = (inputs.T @ mean_error) / len(targets)\n",
    "        \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17e688d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression gradient (dummy data):\n",
      "[[-0.00225]\n",
      " [-0.00914]\n",
      " [-0.06038]\n",
      " [ 0.01138]\n",
      " [ 0.0732 ]]\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy dataset consisting of 10 datapoints\n",
    "dummy_inputs = np.array([\n",
    "    [0.275, 0.239, 0.258, 0.   , 0.   ],\n",
    "    [0.31 , 0.325, 0.223, 0.   , 0.24 ],\n",
    "    [0.37 , 0.324, 1.191, 0.   , 0.   ],\n",
    "    [0.36 , 0.385, 0.821, 0.082, 0.3  ],\n",
    "    [0.41 , 0.519, 0.27 , 0.272, 0.39 ],\n",
    "    [0.4  , 0.536, 0.693, 0.   , 0.   ],\n",
    "    [0.42 , 0.449, 0.586, 0.192, 0.21 ],\n",
    "    [0.39 , 0.345, 0.258, 0.   , 0.3  ],\n",
    "    [0.4  , 0.43 , 0.402, 0.   , 0.   ],\n",
    "    [0.39 , 0.37 , 0.439, 0.022, 0.27 ]\n",
    "])\n",
    "dummy_targets = np.array([0., 0., 1., 0., 0., 1., 1., 0., 1., 0.]).reshape(-1, 1)\n",
    "dummy_logistic_reg_soft_preds = np.array([0.1, 0.4, 0.6, 0.6, 0.8, 0.2, 0.1, 0.3, 0.15, 0.9]).reshape(-1, 1)\n",
    "\n",
    "# Evaluate logistic gradient function\n",
    "dummy_gradient = logistic_regression_gradient(dummy_inputs, dummy_targets, dummy_logistic_reg_soft_preds)\n",
    "print(f\"Logistic regression gradient (dummy data):\\n{dummy_gradient}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2debb7f",
   "metadata": {},
   "source": [
    "### Question 5 [10 points]\n",
    "\n",
    "Complete the function `gradient_descent_step` which takes as input the gradient $\\nabla \\mathcal{L}_{\\mathcal{D}}(\\theta^{(l)})$ evaluated at the current parameter iterate $\\theta^{(l)}$ (also given as input) and a learning rate $\\gamma > 0$; and outputs the next parameter iterate $\\theta^{(i+1)}$ by applying the gradient descent step\n",
    "\\begin{equation*}\n",
    "    \\theta^{(l+1)} = \\theta^{(l)} - \\gamma \\nabla \\mathcal{L}_{\\mathcal{D}}(\\theta^{(l)}).\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e25d6898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_step(theta, gradient, lr):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : np.ndarray\n",
    "        K x 1 parameter vector\n",
    "    \n",
    "    gradient : np.ndarray\n",
    "        K x 1 gradient vector\n",
    "    \n",
    "    lr : float\n",
    "        Scalar learning rate\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    new_theta : np.ndarray\n",
    "        K x 1 parameter vector after gradient-descent update\n",
    "    \"\"\"\n",
    "\n",
    "    # Write your code here\n",
    "    new_theta = theta - lr*gradient\n",
    "    \n",
    "    return new_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a93b34fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dummy parameter:\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Dummy gradient:\n",
      "[[0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [5.]\n",
      " [6.]]\n",
      "Learning rate:0.1\n",
      "New parameter after gradient descent step:\n",
      "[[ 0. ]\n",
      " [-0.1]\n",
      " [-0.2]\n",
      " [-0.3]\n",
      " [-0.4]\n",
      " [-0.5]\n",
      " [-0.6]]\n"
     ]
    }
   ],
   "source": [
    "# Dummy inputs\n",
    "n_dummy_features = 7\n",
    "dummy_theta = np.zeros(n_dummy_features, dtype=float).reshape(-1, 1)\n",
    "dummy_gradient = np.arange(n_dummy_features, dtype=float).reshape(-1, 1)\n",
    "dummy_lr = 0.1\n",
    "\n",
    "# Gradient descent step\n",
    "new_theta = gradient_descent_step(dummy_theta, dummy_gradient, dummy_lr)\n",
    "\n",
    "print(f\"Initial dummy parameter:\\n{dummy_theta}\")\n",
    "print(f\"Dummy gradient:\\n{dummy_gradient}\")\n",
    "print(f\"Learning rate:{dummy_lr}\")\n",
    "print(f\"New parameter after gradient descent step:\\n{new_theta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a65357",
   "metadata": {},
   "source": [
    "### Question 6 [10 points total]\n",
    "\n",
    "#### Question 6.1 [5 points]\n",
    "\n",
    "Complete the function `train_logistic_regression` which takes as input\n",
    "- the data matrix $X_\\mathcal{D}$,\n",
    "- its associated target vector $t_\\mathcal{D}$,\n",
    "- an initial value $\\theta^{(0)}$ of the parameter vector, \n",
    "- a learning rate $\\gamma > 0$\n",
    "- a number of gradient-descent steps $L$;\n",
    "\n",
    "and returns the learned parameter $\\theta^{(L)}$ after $L$ gradient-descent steps on the dataset $\\mathcal{D}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4043eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(inputs, targets, init_theta, lr, n_gd_steps):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs : np.ndarray\n",
    "        N x K data matrix\n",
    "    \n",
    "    targets : np.ndarray\n",
    "        N x 1 target vector\n",
    "        \n",
    "    init_theta : np.ndarray\n",
    "        K x 1 parameter vector initial value\n",
    "    \n",
    "    lr : float\n",
    "        Scalar learning rate\n",
    "    \n",
    "    n_gd_steps : int\n",
    "        Number of gradient-descent steps\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    learned_theta : np.ndarray\n",
    "        K x 1 parameter vector learned after `n_gd_steps` gradient-descent steps\n",
    "    \"\"\"\n",
    "\n",
    "    learned_theta = init_theta\n",
    "    # Write your code here\n",
    "    for i in range(n_gd_steps):\n",
    "        \n",
    "        soft_predictions = sigmoid(np.matmul(inputs, learned_theta)) # Using the sigmoid function defined earlier, \n",
    "        # we can calculate a vector of predictions via matrix multiplication of theta and x\n",
    "        gradient = logistic_regression_gradient(inputs, targets, soft_predictions) # Then use function logistic_gradeint_descent for gradient \n",
    "        learned_theta = gradient_descent_step(learned_theta, gradient, lr) # and use gradient_descent_step for one step\n",
    "\n",
    "    \n",
    "    return learned_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9112793a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned parameter on dummy samples:\n",
      "[[ 0.19873971]\n",
      " [ 0.02518555]\n",
      " [ 0.03270425]\n",
      " [ 0.28887299]\n",
      " [-0.32684191]\n",
      " [ 0.78003004]\n",
      " [ 0.01735104]]\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy dataset consisting of 10 datapoints\n",
    "dummy_inputs = np.array([\n",
    "    [0.233, 0.275, 0.239, 0.258, 0.4  , 0.   , 0.   ],\n",
    "    [0.278, 0.31 , 0.325, 0.223, 0.54 , 0.   , 0.24 ],\n",
    "    [0.433, 0.37 , 0.324, 1.191, 0.985, 0.   , 0.   ],\n",
    "    [0.267, 0.36 , 0.385, 0.821, 0.535, 0.082, 0.3  ],\n",
    "    [0.3  , 0.41 , 0.519, 0.27 , 0.76 , 0.272, 0.39 ],\n",
    "    [0.233, 0.4  , 0.536, 0.693, 0.59 , 0.   , 0.   ],\n",
    "    [0.567, 0.42 , 0.449, 0.586, 0.905, 0.192, 0.21 ],\n",
    "    [0.411, 0.39 , 0.345, 0.258, 0.44 , 0.   , 0.3  ],\n",
    "    [0.489, 0.4  , 0.43 , 0.402, 0.66 , 0.   , 0.   ],\n",
    "    [0.444, 0.39 , 0.37 , 0.439, 0.63 , 0.022, 0.27 ]\n",
    "])\n",
    "dummy_targets = np.array([0., 0., 1., 0., 0., 1., 1., 0., 1., 0.]).reshape(-1, 1)\n",
    "\n",
    "# Init model parameters\n",
    "n_dummy_features = 7\n",
    "dummy_init_theta = np.ones(n_dummy_features, dtype=float).reshape(-1, 1)\n",
    "\n",
    "# Training parameters\n",
    "dummy_n_gd_steps = 100\n",
    "dummy_lr = 0.1\n",
    "\n",
    "# Evaluate function\n",
    "dummy_learned_theta = train_logistic_regression(dummy_inputs, dummy_targets, dummy_init_theta, dummy_lr, dummy_n_gd_steps)\n",
    "\n",
    "print(f\"Learned parameter on dummy samples:\\n{dummy_learned_theta}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56a48de",
   "metadata": {},
   "source": [
    "### Question 6.2 [5 points]\n",
    "\n",
    "Complete the function `train_and_test_logistic_regression` which takes as input\n",
    "- the training data matrix $X_{\\mathcal{D}^\\mathrm{tr}}$,\n",
    "- the training target vector $t_{\\mathcal{D}^\\mathrm{tr}}$,\n",
    "- the test data matrix $X_{\\mathcal{D}^\\mathrm{te}}$,\n",
    "- the test target vector $t_{\\mathcal{D}^\\mathrm{te}}$,\n",
    "- an initial value $\\theta^{(0)}$ of the parameter vector, \n",
    "- a learning rate $\\gamma > 0$\n",
    "- a number of gradient-descent steps $L$;\n",
    "\n",
    "and returns\n",
    "- the learned parameter $\\theta^{(L)}$ after $L$ gradient-descent steps on the dataset $\\mathcal{D}$,\n",
    "- the training loss $\\mathcal{L}_{\\mathcal{D}^\\mathrm{tr}}(\\theta^{(L)})$,\n",
    "- and the test loss $\\mathcal{L}_{\\mathcal{D}^\\mathrm{te}}(\\theta^{(L)})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "446625d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_logistic_regression(inputs_tr, targets_tr, inputs_te, targets_te, init_theta, lr, n_gd_steps):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs_tr : np.ndarray\n",
    "        N_tr x K training data matrix\n",
    "    \n",
    "    targets_tr : np.ndarray\n",
    "        N_tr x 1 training target vector\n",
    "\n",
    "    inputs_te : np.ndarray\n",
    "        N_te x K test data matrix\n",
    "    \n",
    "    targets_te : np.ndarray\n",
    "        N_te x 1 test target vector\n",
    "        \n",
    "    init_theta : np.ndarray\n",
    "        K x 1 parameter vector initial value\n",
    "    \n",
    "    lr : float\n",
    "        Scalar learning rate\n",
    "    \n",
    "    n_gd_steps : int\n",
    "        Number of gradient-descent steps\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    learned_theta : np.ndarray\n",
    "        K x 1 parameter vector learned after `n_gd_steps` gradient-descent steps\n",
    "\n",
    "    loss_tr : float\n",
    "        Scalar cross-entropy training loss\n",
    "\n",
    "    loss_te : float\n",
    "        Scalar cross-entropy test loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Write your code here\n",
    "    \n",
    "    # Use train_logistic_regressions to calculate learned_theta\n",
    "    learned_theta = train_logistic_regression(inputs_tr, targets_tr, init_theta, lr, n_gd_steps) \n",
    "\n",
    "    # Use matmul and sigmoid to calculate soft_predictions_tr and soft_predictions_te\n",
    "    soft_predictions_tr = sigmoid(np.matmul(inputs_tr, learned_theta))\n",
    "    \n",
    "    soft_predictions_te = sigmoid(np.matmul(inputs_te, learned_theta))\n",
    "\n",
    "    # Then use cross_entropy_loss to compute loss_tr and loss_te\n",
    "    loss_tr = cross_entropy_loss(soft_predictions_tr, targets_tr)\n",
    "    \n",
    "    loss_te = cross_entropy_loss(soft_predictions_te, targets_te)\n",
    "\n",
    "    return learned_theta, loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "540a5bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned parameter 'theta' (diabetes dataset):\n",
      "[[ 0.69027026]\n",
      " [-3.76899381]\n",
      " [-0.50650655]\n",
      " [ 0.54826923]\n",
      " [ 2.36417276]\n",
      " [ 0.65256661]\n",
      " [ 0.02322827]]\n",
      "Train loss (diabetes dataset): 0.6164908278244284\n",
      "Estimated population loss for learned parameter (diabetes dataset): 0.6620104796260169\n"
     ]
    }
   ],
   "source": [
    "# Init model parameters\n",
    "n_dataset_features = 7\n",
    "dataset_init_theta = np.ones(n_dataset_features, dtype=float).reshape(-1, 1)\n",
    "\n",
    "# Init learning parameters\n",
    "dataset_n_gd_steps = 20000\n",
    "dataset_lr = 0.05\n",
    "\n",
    "\n",
    "# Train logistic regression and estimate the population loss for the learned parameter\n",
    "dataset_learned_theta, dataset_loss_tr, dataset_loss_te = train_and_test_logistic_regression(\n",
    "    np.copy(dataset_inputs_tr),\n",
    "    np.copy(dataset_targets_tr),\n",
    "    np.copy(dataset_inputs_te),\n",
    "    np.copy(dataset_targets_te),\n",
    "    dataset_init_theta,\n",
    "    dataset_lr,\n",
    "    dataset_n_gd_steps\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Learned parameter 'theta' (diabetes dataset):\\n{dataset_learned_theta}\")\n",
    "print(f\"Train loss (diabetes dataset): {dataset_loss_tr}\")\n",
    "print(f\"Estimated population loss for learned parameter (diabetes dataset): {dataset_loss_te}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada5f71",
   "metadata": {},
   "source": [
    "## Part II\n",
    "\n",
    "In this second section, we will implement a neural network with a single feature extraction layer of $M = 10$ neurons using a ReLu activation function \n",
    "\\begin{equation*}\n",
    "    h(a) = \\max(a, 0).\n",
    "\\end{equation*}\n",
    "Accordingly, the parameters to be learned are $\\theta = \\{W^\\mathrm{feat}, W^\\mathrm{class}\\}$, where $W^\\mathrm{feat}$ denotes the $M \\times K$ matrix of weights for the feature extraction layer, and $W^\\mathrm{class}$ denotes the $1 \\times M$ matrix of weights for the classification layer.\n",
    "For a given $K \\times 1$ covariate vector $x_i$, the neural network computes a soft prediction as follows:\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "    &a^\\mathrm{feat} = W^\\mathrm{feat} x_i \\\\\n",
    "    &h^\\mathrm{feat} = h(a^\\mathrm{feat}) \\\\\n",
    "    &a^\\mathrm{class} = W^\\mathrm{class} h^\\mathrm{feat} \\\\\n",
    "    &p(\\mathrm{t}_i = 1 | x_i, \\theta) = \\sigma(a^\\mathrm{class}),\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "where $a^\\mathrm{feat}$ is the $M \\times 1$ vector of pre-activations for the feature extraction layer, $h^\\mathrm{feat}$ denotes the $M \\times 1$ output of the feature extraction layer, $a^\\mathrm{class}$ is the scalar pre-activation of the classification layer, and $p(\\mathrm{t}_i = 1 | x_i, \\theta)$ is the output of the neural network, where $\\sigma(\\cdot)$ denotes the sigmoid function defined at the beginning of part I.\n",
    "Note that the activation function $h(\\cdot)$ is applied element-wise to the vector $a^\\mathrm{feat}$ in the equations above.\n",
    "\n",
    "\n",
    "## Question 7 [15 points in total]\n",
    "\n",
    "In the following questions, we will implement the forward pass of the neural network step by step.\n",
    "\n",
    "#### Question 7.1 [5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23364689",
   "metadata": {},
   "source": [
    "Complete the function `nn_pre_activation` which takes the input $h$ and the weights $W$ of any of the two defined layers, i.e. $(h, W) \\in \\{(x_i, W^\\mathrm{feat}), (h^\\mathrm{feat}, W^\\mathrm{class})\\}$, and returns the corresponding pre-activation vector $a = W h$, where $a = a^\\mathrm{feat}$ if $(h, W) = (x_i, W^\\mathrm{feat})$, and $a = a^\\mathrm{class}$ if $(h, W) = (h^\\mathrm{feat}, W^\\mathrm{class})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cd42d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_pre_activation(input_layer, layer_weights):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_layer : np.ndarray\n",
    "        L_in x 1 vector representing the layer input (L_in in {K, M})\n",
    "    \n",
    "    layer_weights : np.ndarray\n",
    "        L_out x L_in matrix representing the layer weights ((L_in, L_out) in {(K, M), (M, 1)})\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pre_activation : np.ndarray\n",
    "        L_out x 1 vector of pre-activations\n",
    "    \"\"\"\n",
    "\n",
    "    # Write your code here\n",
    "\n",
    "    # We can use matrix multiplication to find the pre-activation vector\n",
    "    pre_activation = layer_weights @ input_layer\n",
    "    \n",
    "    return pre_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f67921b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy input:\n",
      "[[0.233]\n",
      " [0.275]\n",
      " [0.239]\n",
      " [0.258]\n",
      " [0.4  ]\n",
      " [0.   ]\n",
      " [0.   ]]\n",
      "Dummy layer weights:\n",
      "[[-0.52 -0.05  0.47 -0.37 -0.32  0.02 -0.33]\n",
      " [ 0.29  0.01  0.31 -1.07 -0.07  0.43  0.16]\n",
      " [-0.86  0.6  -0.21 -0.02  0.12  0.04 -0.09]\n",
      " [-0.39  0.76  0.43  0.17  0.97 -0.36 -0.19]\n",
      " [-0.23 -0.37  0.08  0.54 -0.21  0.11 -0.08]\n",
      " [-0.76 -0.18  0.92  0.54  0.54 -0.22  0.46]\n",
      " [ 0.05  0.31  0.93 -0.13 -0.02 -0.01 -1.1 ]\n",
      " [-0.35 -0.58 -0.77 -0.35 -0.37 -0.58  0.17]\n",
      " [ 0.7   0.01  0.8  -1.22 -1.02 -0.35 -0.15]\n",
      " [ 0.42 -0.24  1.3  -0.62  0.54 -0.49  0.08]]\n",
      "Computed pre-activation vector:\n",
      "[[-0.24604]\n",
      " [-0.15965]\n",
      " [-0.04273]\n",
      " [ 0.65276]\n",
      " [-0.0809 ]\n",
      " [ 0.34862]\n",
      " [ 0.27763]\n",
      " [-0.66338]\n",
      " [-0.36571]\n",
      " [ 0.3986 ]]\n"
     ]
    }
   ],
   "source": [
    "dummy_input = np.array([0.233, 0.275, 0.239, 0.258, 0.4, 0., 0.]).reshape(-1, 1)\n",
    "dummy_weights = np.array([\n",
    "    [-0.52, -0.05,  0.47, -0.37, -0.32,  0.02, -0.33],\n",
    "    [ 0.29,  0.01,  0.31, -1.07, -0.07,  0.43,  0.16],\n",
    "    [-0.86,  0.6 , -0.21, -0.02,  0.12,  0.04, -0.09],\n",
    "    [-0.39,  0.76,  0.43,  0.17,  0.97, -0.36, -0.19],\n",
    "    [-0.23, -0.37,  0.08,  0.54, -0.21,  0.11, -0.08],\n",
    "    [-0.76, -0.18,  0.92,  0.54,  0.54, -0.22,  0.46],\n",
    "    [ 0.05,  0.31,  0.93, -0.13, -0.02, -0.01, -1.1 ],\n",
    "    [-0.35, -0.58, -0.77, -0.35, -0.37, -0.58,  0.17],\n",
    "    [ 0.7 ,  0.01,  0.8 , -1.22, -1.02, -0.35, -0.15],\n",
    "    [ 0.42, -0.24,  1.3 , -0.62,  0.54, -0.49,  0.08]\n",
    "])\n",
    "dummy_pre_activation = nn_pre_activation(dummy_input, dummy_weights)\n",
    "\n",
    "\n",
    "print(f\"Dummy input:\\n{dummy_input}\")\n",
    "print(f\"Dummy layer weights:\\n{dummy_weights}\")\n",
    "print(f\"Computed pre-activation vector:\\n{dummy_pre_activation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e3dc0e",
   "metadata": {},
   "source": [
    "#### Question 7.2 [5 points]\n",
    "\n",
    "Complete the function `nn_activation` which takes as input pre-activation vector $a^\\mathrm{feat}$ and returns the output of the feature extraction layer $h^\\mathrm{feat} = h(a^\\mathrm{feat})$, where $h(\\cdot)$ is applied element-wise to the vector $a^\\mathrm{feat}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "581f31e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_activation(pre_activation):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    pre_activation : np.ndarray\n",
    "        M x 1 vector of pre-activations\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    layer_output : np.ndarray\n",
    "        M x 1 vector of element-wise activations (i.e., feature layer output)\n",
    "    \"\"\"\n",
    "\n",
    "    # Write your code here\n",
    "\n",
    "    # h is ReLU, so:\n",
    "    layer_output = np.maximum(pre_activation, 0)\n",
    "    return layer_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18cb5e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy pre-activation vector:\n",
      "[[-0.806]\n",
      " [-0.234]\n",
      " [ 1.106]\n",
      " [ 1.241]\n",
      " [-1.79 ]\n",
      " [ 0.649]\n",
      " [-0.397]\n",
      " [-1.276]\n",
      " [ 0.149]\n",
      " [-0.85 ]]\n",
      "Computed activation vector (output of layer):\n",
      "[[0.   ]\n",
      " [0.   ]\n",
      " [1.106]\n",
      " [1.241]\n",
      " [0.   ]\n",
      " [0.649]\n",
      " [0.   ]\n",
      " [0.   ]\n",
      " [0.149]\n",
      " [0.   ]]\n"
     ]
    }
   ],
   "source": [
    "dummy_pre_activation = np.array([-0.806, -0.234, 1.106, 1.241, -1.79, 0.649, -0.397, -1.276, 0.149, -0.85]).reshape(-1, 1)\n",
    "dummy_output_layer = nn_activation(dummy_pre_activation)\n",
    "\n",
    "\n",
    "print(f\"Dummy pre-activation vector:\\n{dummy_pre_activation}\")\n",
    "print(f\"Computed activation vector (output of layer):\\n{dummy_output_layer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2636b0",
   "metadata": {},
   "source": [
    "#### Question 7.3 [5 points]\n",
    "\n",
    "Complete the function `nn_forward_pass` which takes as input a covariate vector $x_i$, the layers weights $W^\\mathrm{feat}$ and $W^\\mathrm{class}$; and returns the neural network soft prediction $p(\\mathrm{t}_i = 1 | x_i, \\theta)$ for $\\theta = \\{W^\\mathrm{feat}, W^\\mathrm{class}\\}$.\n",
    "\n",
    "**Be mindful of the output shape**: in this coursework, we will assume that the neural network returns the soft-prediction $p(\\mathrm{t}_i = 1 | x_i, \\theta)$ as a scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "436d45d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_forward_pass(nn_input, weights_feat, weights_class):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    nn_input : np.ndarray\n",
    "        K x 1 covariate vector sample\n",
    "    \n",
    "    weights_feat : np.ndarray\n",
    "        M x K matrix representing the weights of the feature extraction layer\n",
    "    \n",
    "    weights_class : np.ndarray\n",
    "        1 x M matrix representing the weights of the classification layer\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    soft_prediction : float\n",
    "        Scalar value representing the neural network soft prediction\n",
    "    \"\"\"\n",
    "\n",
    "    # Write your code here\n",
    "\n",
    "    # Using nn_pre_activation, nn_activation and stepping though system.\n",
    "    \n",
    "    a_feat = nn_pre_activation(nn_input, weights_feat) # Calculate vector a_feat\n",
    "    \n",
    "    h_feat = nn_activation(a_feat) # Apply ReLU to a_feat to get h_feat\n",
    "    \n",
    "    a_class = nn_pre_activation(h_feat, weights_class) # Multiply by weights to get classification layer a_class\n",
    "    \n",
    "    soft_prediction = sigmoid(a_class) # Apply sigmoid to produce soft predictions\n",
    "    \n",
    "    return soft_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3b094d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probabilities of having diabetes (dummy inputs and parameters):\n",
      "[[0.47127544]\n",
      " [0.51334258]\n",
      " [0.4093323 ]\n",
      " [0.46703696]\n",
      " [0.53523326]\n",
      " [0.44427713]\n",
      " [0.47716762]\n",
      " [0.5264456 ]\n",
      " [0.48588795]\n",
      " [0.49776611]]\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy covariates representing of 10 datapoints\n",
    "dummy_inputs = np.array([\n",
    "    [0.233, 0.275, 0.239, 0.258, 0.4  , 0.   , 0.   ],\n",
    "    [0.278, 0.31 , 0.325, 0.223, 0.54 , 0.   , 0.24 ],\n",
    "    [0.433, 0.37 , 0.324, 1.191, 0.985, 0.   , 0.   ],\n",
    "    [0.267, 0.36 , 0.385, 0.821, 0.535, 0.082, 0.3  ],\n",
    "    [0.3  , 0.41 , 0.519, 0.27 , 0.76 , 0.272, 0.39 ],\n",
    "    [0.233, 0.4  , 0.536, 0.693, 0.59 , 0.   , 0.   ],\n",
    "    [0.567, 0.42 , 0.449, 0.586, 0.905, 0.192, 0.21 ],\n",
    "    [0.411, 0.39 , 0.345, 0.258, 0.44 , 0.   , 0.3  ],\n",
    "    [0.489, 0.4  , 0.43 , 0.402, 0.66 , 0.   , 0.   ],\n",
    "    [0.444, 0.39 , 0.37 , 0.439, 0.63 , 0.022, 0.27 ]\n",
    "])\n",
    "\n",
    "# Model parameters\n",
    "dummy_weights_feat = np.array([\n",
    "    [-0.52, -0.05,  0.47, -0.37, -0.32,  0.02, -0.33],\n",
    "    [ 0.29,  0.01,  0.31, -1.07, -0.07,  0.43,  0.16],\n",
    "    [-0.86,  0.6 , -0.21, -0.02,  0.12,  0.04, -0.09],\n",
    "    [-0.39,  0.76,  0.43,  0.17,  0.97, -0.36, -0.19],\n",
    "    [-0.23, -0.37,  0.08,  0.54, -0.21,  0.11, -0.08],\n",
    "    [-0.76, -0.18,  0.92,  0.54,  0.54, -0.22,  0.46],\n",
    "    [ 0.05,  0.31,  0.93, -0.13, -0.02, -0.01, -1.1 ],\n",
    "    [-0.35, -0.58, -0.77, -0.35, -0.37, -0.58,  0.17],\n",
    "    [ 0.7 ,  0.01,  0.8 , -1.22, -1.02, -0.35, -0.15],\n",
    "    [ 0.42, -0.24,  1.3 , -0.62,  0.54, -0.49,  0.08]\n",
    "])\n",
    "dummy_weights_class = np.array([\n",
    "    [ 1.96, 0.88, 0.1, -0.51, 1.24, -0.1, -0.31, -0.96, 0.05, 0.85]\n",
    "])\n",
    "\n",
    "# Compute forward pass for each input\n",
    "dummy_outputs = []\n",
    "for i in range(len(dummy_inputs)):\n",
    "    input_vector = dummy_inputs[i].reshape(-1, 1)\n",
    "    soft_pred = nn_forward_pass(input_vector, dummy_weights_feat, dummy_weights_class)\n",
    "    dummy_outputs.append(soft_pred)\n",
    "dummy_outputs = np.array(dummy_outputs).reshape(-1, 1)\n",
    "\n",
    "print(f\"Predicted probabilities of having diabetes (dummy inputs and parameters):\\n{dummy_outputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5f59c9",
   "metadata": {},
   "source": [
    "### Question 8 [25 points in total]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9984d5",
   "metadata": {},
   "source": [
    "In the following questions, we will implement the backward pass of the neural network step by step.\n",
    "The objective is to compute the derivatives of the log-loss $\\ell_i(\\theta) = -\\log(p(\\mathrm{t}_i = t_i | x_i, \\theta))$ with respect to each element of $\\theta = \\{W^\\mathrm{feat}, W^\\mathrm{class}\\}$ for a given datapoint $(x_i, t_i) \\in \\mathcal{D}$.\n",
    "Accordingly, we will write the derivatives of $\\ell_i(\\theta)$ with respect to each weight of the matrix $W^\\mathrm{feat} = [w^\\mathrm{feat}_{k, m}]_{(k, m) \\in \\{0, ..., K-1\\} \\times \\{0, ..., M-1\\}}$ as the $M \\times K$ matrix\n",
    "\\begin{equation*}\n",
    "    \\nabla_{W^\\mathrm{feat}} \\ell_i(\\theta) = \\begin{bmatrix}\n",
    "        \\frac{\\partial \\ell_i(\\theta)}{\\partial w^\\mathrm{feat}_{0, 0}} & \\dots & \\frac{\\partial \\ell_i(\\theta)}{\\partial w^\\mathrm{feat}_{0, K-1}} \\\\\n",
    "        \\vdots & \\ddots & \\vdots \\\\\n",
    "        \\frac{\\partial \\ell_i(\\theta)}{\\partial w^\\mathrm{feat}_{M-1, 0}} & \\dots & \\frac{\\partial \\ell_i(\\theta)}{\\partial w^\\mathrm{feat}_{M-1, K-1}}\n",
    "    \\end{bmatrix}.\n",
    "\\end{equation*}\n",
    "Similarly, we define the $1 \\times M$ row vector $\\nabla_{W^\\mathrm{class}} \\ell_i(\\theta)$ of the derivatives of $\\ell_i(\\theta)$ with respect to the elements of $W^\\mathrm{class}$.\n",
    "Note that $\\ell_i(\\theta)$ is sometimes (equivalently) expressed as $\\ell_i(\\theta) = -\\log(\\sigma(\\mathrm{t}^{\\pm}_i a^\\mathrm{class}))$ in the lecture slides.\n",
    "\n",
    "We denote as\n",
    "\\begin{equation*}\n",
    "    \\delta^\\mathrm{class} = \\frac{\\partial \\ell_i(\\theta)}{\\partial a^\\mathrm{class}}\n",
    "\\end{equation*}\n",
    "the scalar backpropagation error of the classification layer, and as\n",
    "\\begin{equation*}\n",
    "    \\delta^\\mathrm{feat} = \\left[ \\frac{\\partial \\ell_i(\\theta)}{\\partial a^\\mathrm{feat}_0}, ..., \\frac{\\partial \\ell_i(\\theta)}{\\partial a^\\mathrm{feat}_{M-1}} \\right]^\\top\n",
    "\\end{equation*}\n",
    "the $M \\times 1$ backpropagation error vector of the feature extraction layer, for $a^\\mathrm{feat} = [a^\\mathrm{feat}_0, ..., a^\\mathrm{feat}_{M-1}]^\\top$.\n",
    "Following the backward version of the neural network computational graph, these two quantities can be related as\n",
    "\\begin{equation*}\n",
    "    \\delta^\\mathrm{feat} = h'(a^\\mathrm{feat}) \\odot \\left( (W^\\mathrm{class})^\\top \\delta^\\mathrm{class} \\right),\n",
    "\\end{equation*}\n",
    "where $h'(a^\\mathrm{feat}) = {dh(a^\\mathrm{feat})} / {da^\\mathrm{feat}}$ is the derivative of the activation function $h(\\cdot)$ evaluated element-wise at $a^\\mathrm{feat}$, and $\\odot$ denotes the element-wise multiplication of two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45553f1",
   "metadata": {},
   "source": [
    "#### Question 8.1 [5 points]\n",
    "\n",
    "Complete the function `nn_activation_derivative` which takes as input the pre-activation vector $a^\\mathrm{feat}$ and outputs the ReLu derivative evaluated element-wise at $a^\\mathrm{feat}$, i.e., $h'(a^\\mathrm{feat}) = [h'(a^\\mathrm{feat}_0), ..., h'(a^\\mathrm{feat}_{M-1})]^\\top$.\n",
    "The derivative $h'(\\cdot)$ must be computed using **symbolic differentiation** (and **not** using numerical differentiation).\n",
    "Note that the derivative at $0$ is undefined, and we will assume it to be equal to $0$ in this coursework, i.e., $h'(0) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "094a141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_activation_derivative(pre_activation):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    pre_activation : np.ndarray\n",
    "        M x 1 vector of pre-activations\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    activation_derivative : np.ndarray\n",
    "        M x 1 vector of activation derivatives evaluated at each element of the `pre_activation` vector\n",
    "    \"\"\"\n",
    "\n",
    "    # Write your code here\n",
    "\n",
    "    # Derivative of ReLU = 1 if a > 0, otherwise 0.\n",
    "    \n",
    "    activation_derivative = (pre_activation > 0).astype(int)\n",
    "    return activation_derivative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f17c1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy pre-activation vector:\n",
      "[[-0.806]\n",
      " [-0.234]\n",
      " [ 1.106]\n",
      " [ 1.241]\n",
      " [-1.79 ]\n",
      " [ 0.649]\n",
      " [-0.397]\n",
      " [-1.276]\n",
      " [ 0.149]\n",
      " [-0.85 ]]\n",
      "Computed activation-derivative vector:\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "dummy_pre_activation = np.array([-0.806, -0.234, 1.106, 1.241, -1.79 , 0.649, -0.397, -1.276, 0.149, -0.85]).reshape(-1, 1)\n",
    "\n",
    "dummy_activation_derivative = nn_activation_derivative(dummy_pre_activation)\n",
    "\n",
    "\n",
    "print(f\"Dummy pre-activation vector:\\n{dummy_pre_activation}\")\n",
    "print(f\"Computed activation-derivative vector:\\n{dummy_activation_derivative}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466adb93",
   "metadata": {},
   "source": [
    "#### Question 8.2 [5 points]\n",
    "\n",
    "Complete the function `nn_backprop_error_classification_layer` which takes as input the soft prediction $p(\\mathrm{t}_i = 1 | x_i, \\theta)$ computed during the forward pass and the target value $t_i$; and returns the backpropagation error at the classification layer $\\delta^\\mathrm{class}$.\n",
    "\n",
    "**Take a careful look at the specified output shape**: in this coursework, we will consider the backpropagation error at the classification layer $\\delta^\\mathrm{class}$ to be specified as a $1 \\times 1$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb68d1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_backprop_error_classification_layer(soft_prediction, target):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    soft_prediction : float\n",
    "        Scalar value representing the neural network soft prediction\n",
    "    \n",
    "    target : int\n",
    "        Scalar target value (`1` if the patient has diabetes, otherwise `0`)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    backprop_error_class : np.ndarray\n",
    "        1 x 1 matrix representing the backpropagation error at the classification layer\n",
    "    \"\"\"\n",
    "\n",
    "    # Write your code here\n",
    "\n",
    "    backprop_error_class = (soft_prediction - target) # Calculate the error\n",
    "    \n",
    "    return np.array(backprop_error_class).reshape(1,1) # Make the error a numpy array and then reshape to get 1 x 1 matrix\n",
    " \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8438fd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed backpropagation error at the classification layer (dummy inputs):\n",
      "[[-0.28]]\n"
     ]
    }
   ],
   "source": [
    "dummy_soft_prediction = 0.72\n",
    "dummy_target = 1\n",
    "\n",
    "dummy_backprop_error_class = nn_backprop_error_classification_layer(dummy_soft_prediction, dummy_target)\n",
    "\n",
    "\n",
    "print(f\"Computed backpropagation error at the classification layer (dummy inputs):\\n{dummy_backprop_error_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3781b611",
   "metadata": {},
   "source": [
    "#### Question 8.3 [5 points]\n",
    "\n",
    "Complete the function `nn_backprop_error_feature_layer` which takes as input\n",
    "- the element-wise activation derivatives $h'(a^\\mathrm{feat})$,\n",
    "- the weights $W^\\mathrm{class}$ of the classification layer,\n",
    "- the backpropagation error at the classification layer $\\delta^\\mathrm{class}$;\n",
    "\n",
    "and returns the backpropagation error at the feature extraction layer $\\delta^\\mathrm{feat}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e86f67a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_backprop_error_feature_layer(activation_derivative, weights_class, backprop_error_class):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    activation_derivative : np.ndarray\n",
    "        M x 1 vector of activation derivatives\n",
    "    \n",
    "    weights_class : np.ndarray\n",
    "        1 x M matrix representing the weights of the classification layer\n",
    "    \n",
    "    backprop_error_class : np.ndarray\n",
    "        1 x 1 matrix representing the backpropagation error at the classification layer\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    backprop_error_feat : np.ndarray\n",
    "        M x 1 vector representing the backpropagation error at the feature layer\n",
    "    \"\"\"\n",
    "\n",
    "    # Write your code here\n",
    "\n",
    "    backprop_error_feat = activation_derivative * (weights_class.T * backprop_error_class)\n",
    "\n",
    "    return backprop_error_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "729e2488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed backpropagation error at the feature layer (dummy inputs and parameters):\n",
      "[[-0.   ]\n",
      " [-0.   ]\n",
      " [-0.05 ]\n",
      " [ 0.   ]\n",
      " [-0.62 ]\n",
      " [ 0.05 ]\n",
      " [ 0.155]\n",
      " [ 0.   ]\n",
      " [-0.   ]\n",
      " [-0.425]]\n"
     ]
    }
   ],
   "source": [
    "dummy_activation_derivative = np.array([0., 0., 1., 0., 1., 1., 1., 0., 0., 1.]).reshape(-1, 1)\n",
    "dummy_weights_class = np.array([\n",
    "    [ 1.96, 0.88, 0.1, -0.51, 1.24, -0.1, -0.31, -0.96, 0.05, 0.85]\n",
    "])\n",
    "dummy_backprop_error_class = np.array([[-0.5]])\n",
    "\n",
    "dummy_backprop_error_feat = nn_backprop_error_feature_layer(dummy_activation_derivative, dummy_weights_class, dummy_backprop_error_class)\n",
    "\n",
    "\n",
    "print(f\"Computed backpropagation error at the feature layer (dummy inputs and parameters):\\n{dummy_backprop_error_feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33e0a09",
   "metadata": {},
   "source": [
    "#### Question 8.4 [5 points]\n",
    "\n",
    "Complete the function `nn_weights_gradient` which, for any of the two defined layers, takes its input $h \\in \\{x_i, h^\\mathrm{feat}\\}$ computed during the forward pass and its corresponding backpropagation error $\\delta \\in \\{\\delta^\\mathrm{feat}, \\delta^\\mathrm{class}\\}$ computed during the backward pass; and returns the gradient $\\nabla_W l_i(\\theta)$ with respect to the weights $W \\in \\{W^\\mathrm{feat}, W^\\mathrm{class}\\}$ of the considered layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "586c2ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_weights_gradient(input_layer, backprop_error_layer):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_layer : np.ndarray\n",
    "        L_in x 1 vector representing the layer input (L_in in {K, M})\n",
    "    \n",
    "    backprop_error_layer : np.ndarray\n",
    "        L_out x 1 matrix representing the backpropagation error at the layer (L_out in {M, 1})\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    weights_gradient : np.ndarray\n",
    "        L_out x L_in matrix representing the gradient of the log-loss with respect to the weights of the layer ((L_in, L_out) in {(K, M), (M, 1)})\n",
    "    \"\"\"\n",
    "\n",
    "    # Write your code here\n",
    "    \n",
    "    # calculate weights_gradient as matrix multiplication of input layer and backprop error\n",
    "    weights_gradient = backprop_error_layer @ input_layer.T \n",
    "    return weights_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25e3b689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed gradient for weights at classification layer (dummy inputs):\n",
      "[[ 0.      0.     -0.553  -0.6205  0.     -0.3245  0.      0.     -0.0745\n",
      "   0.    ]]\n"
     ]
    }
   ],
   "source": [
    "# Gradient at classificaiton layer\n",
    "dummy_input_class = np.array([0.0, 0.0, 1.106, 1.241, 0.0 , 0.649, 0.0, 0.0, 0.149, 0.0]).reshape(-1, 1)\n",
    "dummy_backprop_error_class = np.array([[-0.5]])\n",
    "\n",
    "dummy_weights_class_gradient = nn_weights_gradient(dummy_input_class, dummy_backprop_error_class)\n",
    "\n",
    "\n",
    "print(f\"Computed gradient for weights at classification layer (dummy inputs):\\n{dummy_weights_class_gradient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c6733bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed gradient for weights at feature layer (dummy inputs):\n",
      "[[ 0.       0.       0.       0.       0.     ]\n",
      " [ 0.0252   0.02695  0.05747  0.03745  0.021  ]\n",
      " [ 0.2052   0.21945  0.46797  0.30495  0.171  ]\n",
      " [ 0.       0.       0.       0.       0.     ]\n",
      " [ 0.       0.       0.       0.       0.     ]\n",
      " [ 0.       0.       0.       0.       0.     ]\n",
      " [-0.0936  -0.1001  -0.21346 -0.1391  -0.078  ]\n",
      " [-0.2916  -0.31185 -0.66501 -0.43335 -0.243  ]\n",
      " [-0.1152  -0.1232  -0.26272 -0.1712  -0.096  ]\n",
      " [ 0.27     0.28875  0.61575  0.40125  0.225  ]]\n"
     ]
    }
   ],
   "source": [
    "# Gradient at feature layer\n",
    "dummy_input_feat = np.array([ 0.36, 0.385, 0.821, 0.535, 0.3]).reshape(-1, 1)\n",
    "dummy_backprop_error_feat = np.array([0., 0.07, 0.57, 0., 0., 0., -0.26, -0.81, -0.32, 0.75]).reshape(-1, 1)\n",
    "\n",
    "dummy_weights_feat_gradient = nn_weights_gradient(dummy_input_feat, dummy_backprop_error_feat)\n",
    "\n",
    "\n",
    "print(f\"Computed gradient for weights at feature layer (dummy inputs):\\n{dummy_weights_feat_gradient}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fc87da",
   "metadata": {},
   "source": [
    "#### Question 8.5 [5 points]\n",
    "\n",
    "Complete the function `nn_forward_and_backward_pass` which takes as input \n",
    "- the vector of covariates $x_i$,\n",
    "- the weights $W^\\mathrm{feat}$ and $W^\\mathrm{class}$ of both layers,\n",
    "- the target $t_i$;\n",
    "\n",
    "and returns\n",
    "- the soft prediction $p(\\mathrm{t}_i = 1 | x_i, \\theta)$ computed during a forward pass,\n",
    "- the gradient of the log-loss with respect to the weights of the feature extraction layer $\\nabla_{W^\\mathrm{feat}} l_i(\\theta)$,\n",
    "- the gradient of the log-loss with respect to the weights of the classification layer $\\nabla_{W^\\mathrm{class}} l_i(\\theta)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db2014c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_forward_and_backward_pass(nn_input, weights_feat, weights_class, target):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    nn_input : np.ndarray\n",
    "        K x 1 covariate vector sample\n",
    "    \n",
    "    weights_feat : np.ndarray\n",
    "        M x K matrix representing the weights of the feature extraction layer\n",
    "    \n",
    "    weights_class : np.ndarray\n",
    "        1 x M matrix representing the weights of the classification layer\n",
    "    \n",
    "    target : int\n",
    "        Scalar target value (`1` if the patient has diabetes, otherwise `0`)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    soft_prediction : float\n",
    "        Scalar value representing the neural network soft prediction\n",
    "    \n",
    "    weights_feat_grad : np.ndarray\n",
    "        M x K matrix representing the gradient of the log-loss with respect to the weights of the feature extraction layer\n",
    "    \n",
    "    weights_class_grad : np.ndarray\n",
    "        1 x M matrix representing the gradient of the log-loss with respect to the weights of the classification layer\n",
    "    \"\"\"\n",
    "\n",
    "    # Write your code here\n",
    "\n",
    "    # Forward Pass:\n",
    "\n",
    "    # Matrix multiplication to calculate a_feat\n",
    "    a_feat = weights_feat @ nn_input\n",
    "\n",
    "    # Apply ReLU\n",
    "    h_feat = np.maximum(a_feat, 0)\n",
    "\n",
    "    # Calculate a_class\n",
    "    a_class = np.dot(weights_class, h_feat)\n",
    "\n",
    "    # Apply sigmoid to get soft_prediction\n",
    "    soft_prediction = 1/(1+np.exp(-a_class))\n",
    "\n",
    "    # Using back propagation to calculate gradients\n",
    "    error = soft_prediction - target\n",
    "    \n",
    "    weights_class_grad = error*h_feat.T\n",
    "    \n",
    "    # Backprop through ReLU to find gradient of hidden features\n",
    "    h_grad = error * weights_class.T * (h_feat > 0)\n",
    "\n",
    "    weights_feat_grad = np.dot(h_grad, nn_input.T)\n",
    "\n",
    "    return soft_prediction, weights_feat_grad, weights_class_grad\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "104df102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy input:\n",
      "[[0.233]\n",
      " [0.275]\n",
      " [0.239]\n",
      " [0.258]\n",
      " [0.4  ]\n",
      " [0.   ]\n",
      " [0.   ]]\n",
      "Dummy target:\n",
      "1\n",
      "Predicted probability of having diabetes (dummy parameters):\n",
      "[[0.47127544]]\n",
      "Gradient of the weights at the feature extraction layer (dummy parameters):\n",
      "[[-0.         -0.         -0.         -0.         -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.06282834  0.07415362  0.06444624  0.06956958  0.10785981  0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.         -0.\n",
      "  -0.        ]\n",
      " [ 0.01231928  0.01453993  0.01263652  0.01364109  0.02114898  0.\n",
      "   0.        ]\n",
      " [ 0.03818978  0.04507377  0.0391732   0.04228739  0.06556185  0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.1047139  -0.12358937 -0.10741039 -0.1159493  -0.17976635 -0.\n",
      "  -0.        ]]\n",
      "Gradient of the weights at the classification layer (dummy parameters):\n",
      "[[-0.         -0.         -0.         -0.34513024 -0.         -0.18432396\n",
      "  -0.1467898  -0.         -0.         -0.21074961]]\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy inputs sample\n",
    "dummy_input = np.array([0.233, 0.275, 0.239, 0.258, 0.4, 0., 0.]).reshape(-1, 1)\n",
    "dummy_target = 1\n",
    "\n",
    "# Model parameters\n",
    "dummy_weights_feat = np.array([\n",
    "    [-0.52, -0.05,  0.47, -0.37, -0.32,  0.02, -0.33],\n",
    "    [ 0.29,  0.01,  0.31, -1.07, -0.07,  0.43,  0.16],\n",
    "    [-0.86,  0.6 , -0.21, -0.02,  0.12,  0.04, -0.09],\n",
    "    [-0.39,  0.76,  0.43,  0.17,  0.97, -0.36, -0.19],\n",
    "    [-0.23, -0.37,  0.08,  0.54, -0.21,  0.11, -0.08],\n",
    "    [-0.76, -0.18,  0.92,  0.54,  0.54, -0.22,  0.46],\n",
    "    [ 0.05,  0.31,  0.93, -0.13, -0.02, -0.01, -1.1 ],\n",
    "    [-0.35, -0.58, -0.77, -0.35, -0.37, -0.58,  0.17],\n",
    "    [ 0.7 ,  0.01,  0.8 , -1.22, -1.02, -0.35, -0.15],\n",
    "    [ 0.42, -0.24,  1.3 , -0.62,  0.54, -0.49,  0.08]\n",
    "])\n",
    "dummy_weights_class = np.array([\n",
    "    [ 1.96, 0.88, 0.1, -0.51, 1.24, -0.1, -0.31, -0.96, 0.05, 0.85]\n",
    "])\n",
    "\n",
    "# Compute forward pass for each input\n",
    "dummy_soft_pred, dummy_weights_feat_grad, dummy_weights_class_grad = nn_forward_and_backward_pass(\n",
    "    dummy_input,\n",
    "    dummy_weights_feat,\n",
    "    dummy_weights_class,\n",
    "    dummy_target\n",
    ")\n",
    "\n",
    "print(f\"Dummy input:\\n{dummy_input}\")\n",
    "print(f\"Dummy target:\\n{dummy_target}\")\n",
    "print(f\"Predicted probability of having diabetes (dummy parameters):\\n{dummy_soft_pred}\")\n",
    "print(f\"Gradient of the weights at the feature extraction layer (dummy parameters):\\n{dummy_weights_feat_grad}\")\n",
    "print(f\"Gradient of the weights at the classification layer (dummy parameters):\\n{dummy_weights_class_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075e940a",
   "metadata": {},
   "source": [
    "### Question 9 [10 points total]\n",
    "\n",
    "#### Question 9.1 [5 points]\n",
    "\n",
    "Complete the function `train_neural_network` which takes as input\n",
    "- the data matrix $X_\\mathcal{D}$,\n",
    "- its associated target vector $t_\\mathcal{D}$,\n",
    "- an initial value $W^{\\mathrm{feat}, (0, 0)}$ of the weights at the feature extraction layer,\n",
    "- an initial value $W^{\\mathrm{class}, (0, 0)}$ of the weights at the classification layer,\n",
    "- a learning rate $\\gamma > 0$\n",
    "- a number of _epochs_ $J \\geq 1$;\n",
    "\n",
    "and returns the learned weights $W^{\\mathrm{feat}, (J-1, N)}$ and $W^{\\mathrm{class}, (J-1, N)}$ after $J \\cdot N$ gradient-descent steps over $J$ epochs.\n",
    "\n",
    "Each epoch corresponds to a pass over the dataset $\\mathcal{D}$ using a (**non-random**) consecutive sequence minibatches $\\{(x_i, t_i)\\}$ of size $1$ for each gradient update.\n",
    "More specifically, the $i$-th gradient update at the $j$-th epoch is given as\n",
    "\\begin{equation*}\n",
    "    \\begin{cases}\n",
    "    W^{\\mathrm{feat}, (j, i+1)} = W^{\\mathrm{feat}, (j, i)} - \\gamma \\nabla_{W^\\mathrm{feat}} l_i(\\theta^{(j, i)}) \\\\\n",
    "    W^{\\mathrm{class}, (j, i+1)} = W^{\\mathrm{class}, (j, i)} - \\gamma \\nabla_{W^\\mathrm{class}} l_i(\\theta^{(j, i)})\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "for $i \\in \\{0, ..., N-1\\}$, $j \\in \\{0, ..., J-1\\}$, and $\\theta^{(j, i)} = \\{W^{\\mathrm{feat}, (j, i)}, W^{\\mathrm{class}, (j, i)}\\}$.\n",
    "The last value of the current epoch becomes the initial value of the next epoch, that is $\\theta^{(j, N)} = \\theta^{(j+1, 0)}$ for $j \\in \\{0, ..., J-2\\}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15fd1c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(inputs, targets, init_weights_feat, init_weights_class, lr, n_epochs):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs : np.ndarray\n",
    "        N x K data matrix\n",
    "    \n",
    "    targets : np.ndarray\n",
    "        N x 1 target vector\n",
    "        \n",
    "    init_weights_feat : np.ndarray\n",
    "        M x K matrix representing the initial weights of the feature extraction layer\n",
    "        \n",
    "    init_weights_feat : np.ndarray\n",
    "        1 x M matrix representing the initial weights of the classification layer\n",
    "    \n",
    "    lr : float\n",
    "        Scalar learning rate\n",
    "    \n",
    "    n_epochs : int\n",
    "        Number of epochs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    learned_weights_feat : np.ndarray\n",
    "        M x K matrix representing the learned weights of the feature extraction layer after `N * n_epochs` gradient-descent steps\n",
    "    \n",
    "    learned_weights_class : np.ndarray\n",
    "        1 x M matrix representing the learned weights of the classification layer after `N * n_epochs` gradient-descent steps\n",
    "    \"\"\"\n",
    "\n",
    "    # Write your code here\n",
    "\n",
    "    weights_feat = init_weights_feat\n",
    "    weights_class = init_weights_class\n",
    "\n",
    "    for i in range(n_epochs): \n",
    "        for idx in range(len(targets)):\n",
    "            target = targets[idx] # Get target\n",
    "            nn_input = inputs[idx,:].reshape(-1,1) # Get corrosponding inputs\n",
    "            \n",
    "            # Use function nn_forward_and_backward_pass to compute soft_prediction, weights_feat_grad and weights_class_grad\n",
    "            soft_prediction, weights_feat_grad, weights_class_grad = nn_forward_and_backward_pass(nn_input, weights_feat, weights_class, target)\n",
    "            \n",
    "            # Update weights_feat and weights_class\n",
    "            weights_feat = weights_feat - lr*weights_feat_grad\n",
    "            weights_class = weights_class - lr*weights_class_grad\n",
    "\n",
    "    # We don't need to use np.copy() because we are not going to modify further\n",
    "    learned_weights_feat = weights_feat \n",
    "    learned_weights_class = weights_class\n",
    "\n",
    "    return learned_weights_feat, learned_weights_class\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "022fad7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned weights at the feature extraction layer (dummy dataset):\n",
      "[[ 6.56076243 -5.11561887 -0.78382091  1.81270011 -1.00434546 -5.4941999\n",
      "  -1.10922616]\n",
      " [ 0.03045942  0.0133025  -0.57462159  0.07625212  0.06554856  0.33\n",
      "  -0.57      ]\n",
      " [ 0.53       -0.4        -0.32        0.11       -0.51        0.33\n",
      "  -0.44      ]\n",
      " [-0.19        0.18        0.45       -0.58        0.03        0.31\n",
      "  -0.55      ]\n",
      " [-0.40187122  4.40830225  3.65632565 -0.1158413  -5.53613456 -1.1439153\n",
      "   2.4539135 ]\n",
      " [-2.43100814  0.40944219  0.99017986 -2.23846213  1.98877758 -2.45975853\n",
      "   3.83793263]\n",
      " [-0.36244435  2.500576   -0.84207685  1.71983894 -0.06408059  2.24890808\n",
      "  -1.53569447]\n",
      " [-0.39        0.07        0.08       -0.57       -0.2        -0.35\n",
      "  -0.43      ]\n",
      " [-0.16       -0.05       -0.49       -0.51        0.57        0.35\n",
      "  -0.43      ]\n",
      " [ 0.58       -0.48       -0.33       -0.43       -0.36       -0.11\n",
      "  -0.44      ]]\n",
      "Learned weights at the classification layer (dummy dataset):\n",
      "[[-8.75188735  0.27864714  0.55        0.17        7.66568961 -5.28596511\n",
      "   3.41068548  0.69        0.46       -0.52      ]]\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy dataset representing of 10 datapoints\n",
    "dummy_inputs = np.array([\n",
    "    [0.233, 0.275, 0.239, 0.258, 0.4  , 0.   , 0.   ],\n",
    "    [0.278, 0.31 , 0.325, 0.223, 0.54 , 0.   , 0.24 ],\n",
    "    [0.433, 0.37 , 0.324, 1.191, 0.985, 0.   , 0.   ],\n",
    "    [0.267, 0.36 , 0.385, 0.821, 0.535, 0.082, 0.3  ],\n",
    "    [0.3  , 0.41 , 0.519, 0.27 , 0.76 , 0.272, 0.39 ],\n",
    "    [0.233, 0.4  , 0.536, 0.693, 0.59 , 0.   , 0.   ],\n",
    "    [0.567, 0.42 , 0.449, 0.586, 0.905, 0.192, 0.21 ],\n",
    "    [0.411, 0.39 , 0.345, 0.258, 0.44 , 0.   , 0.3  ],\n",
    "    [0.489, 0.4  , 0.43 , 0.402, 0.66 , 0.   , 0.   ],\n",
    "    [0.444, 0.39 , 0.37 , 0.439, 0.63 , 0.022, 0.27 ]\n",
    "])\n",
    "dummy_targets = np.array([1, 0, 0, 1, 0, 1, 1, 1, 0, 0], dtype=float).reshape(-1, 1)\n",
    "\n",
    "# Init model parameters\n",
    "dummy_init_weights_feat = np.array([\n",
    "    [ 0.47,  0.13, -0.49,  0.25,  0.06, -0.08, -0.09],\n",
    "    [ 0.05,  0.03, -0.56,  0.13,  0.11,  0.33, -0.57],\n",
    "    [ 0.53, -0.4 , -0.32,  0.11, -0.51,  0.33, -0.44],\n",
    "    [-0.19,  0.18,  0.45, -0.58,  0.03,  0.31, -0.55],\n",
    "    [-0.33,  0.54, -0.2 ,  0.56,  0.07, -0.56,  0.53],\n",
    "    [ 0.37,  0.4 ,  0.03,  0.05,  0.4 ,  0.44,  0.58],\n",
    "    [ 0.5 ,  0.41, -0.18, -0.01,  0.19,  0.39, -0.32],\n",
    "    [-0.39,  0.07,  0.08, -0.57, -0.2 , -0.35, -0.43],\n",
    "    [-0.16, -0.05, -0.49, -0.51,  0.57,  0.35, -0.43],\n",
    "    [ 0.58, -0.48, -0.33, -0.43, -0.36, -0.11, -0.44]\n",
    "])\n",
    "dummy_init_weights_class = np.array([\n",
    "    [-0.2 ,  0.29,  0.55,  0.17,  0.3 , -0.19,  0.38,  0.69,  0.46, -0.52]\n",
    "])\n",
    "\n",
    "# Training parameters\n",
    "dummy_n_epochs = 1000\n",
    "dummy_lr = 0.1\n",
    "\n",
    "\n",
    "# Train neural network using dummy dataset\n",
    "dummy_learned_weights_feat, dummy_learned_weights_class = train_neural_network(\n",
    "    dummy_inputs,\n",
    "    dummy_targets,\n",
    "    dummy_init_weights_feat,\n",
    "    dummy_init_weights_class,\n",
    "    dummy_lr,\n",
    "    dummy_n_epochs\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Learned weights at the feature extraction layer (dummy dataset):\\n{dummy_learned_weights_feat}\")\n",
    "print(f\"Learned weights at the classification layer (dummy dataset):\\n{dummy_learned_weights_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd0bc8",
   "metadata": {},
   "source": [
    "#### Question 9.2 [5 points]\n",
    "\n",
    "Complete the function `train_and_test_neural_network` which takes as input\n",
    "- the training data matrix $X_{\\mathcal{D}^\\mathrm{tr}}$,\n",
    "- the training target vector $t_{\\mathcal{D}^\\mathrm{tr}}$,\n",
    "- the test data matrix $X_{\\mathcal{D}^\\mathrm{te}}$,\n",
    "- the test target vector $t_{\\mathcal{D}^\\mathrm{te}}$,\n",
    "- an initial value $W^{\\mathrm{feat}, (0, 0)}$ of the weights at the feature extraction layer,\n",
    "- an initial value $W^{\\mathrm{class}, (0, 0)}$ of the weights at the classification layer,\n",
    "- a learning rate $\\gamma > 0$\n",
    "- a number of epochs $J \\geq 1$;\n",
    "\n",
    "and returns\n",
    "- the learned weights $W^{\\mathrm{feat}, (J-1, N^\\mathrm{tr})}$ and $W^{\\mathrm{class}, (J-1, N^\\mathrm{tr})}$ after $J \\cdot N^\\mathrm{tr}$ gradient-descent steps over $J$ epochs,\n",
    "- the training loss $\\mathcal{L}_{\\mathcal{D}^\\mathrm{tr}}(\\theta^{(J-1, N^\\mathrm{tr})})$,\n",
    "- and the test loss $\\mathcal{L}_{\\mathcal{D}^\\mathrm{te}}(\\theta^{(J-1, N^\\mathrm{tr})})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a411968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_neural_network(inputs_tr, targets_tr, inputs_te, targets_te, init_weights_feat, init_weights_class, lr, n_epochs):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs_tr : np.ndarray\n",
    "        N_tr x K training data matrix\n",
    "    \n",
    "    targets_tr : np.ndarray\n",
    "        N_tr x 1 training target vector\n",
    "\n",
    "    inputs_te : np.ndarray\n",
    "        N_te x K test data matrix\n",
    "    \n",
    "    targets_te : np.ndarray\n",
    "        N_te x 1 test target vector\n",
    "        \n",
    "    init_weights_feat : np.ndarray\n",
    "        M x K matrix representing the initial weights of the feature extraction layer\n",
    "        \n",
    "    init_weights_class : np.ndarray\n",
    "        1 x M matrix representing the initial weights of the classification layer\n",
    "    \n",
    "    lr : float\n",
    "        Scalar learning rate\n",
    "    \n",
    "    n_epochs : int\n",
    "        Number of epochs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    learned_weights_feat : np.ndarray\n",
    "        M x K matrix representing the learned weights of the feature extraction layer after `N * n_epochs` gradient-descent steps\n",
    "    \n",
    "    learned_weights_class : np.ndarray\n",
    "        1 x M matrix representing the learned weights of the classification layer after `N * n_epochs` gradient-descent steps\n",
    "\n",
    "    loss_tr : float\n",
    "        Scalar cross-entropy training loss\n",
    "\n",
    "    loss_te : float\n",
    "        Scalar cross-entropy test loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Write your code here\n",
    "\n",
    "    # Use previous function to train the neural network:\n",
    "    \n",
    "    learned_weights_feat, learned_weights_class = train_neural_network(inputs_tr, targets_tr, init_weights_feat, init_weights_class, lr, n_epochs)\n",
    "    \n",
    "    # calculate soft predictions:\n",
    "    \n",
    "    a_feat_mat_te = learned_weights_feat @ inputs_te.T # Produces a matrix M x N_te, where each column represents the a_feat of a datapoint \n",
    "    \n",
    "    # Apply ReLU\n",
    "    h_feat_mat_te = np.maximum(a_feat_mat_te, 0) \n",
    "\n",
    "    # Calculate a_class\n",
    "    a_class_te = learned_weights_class @ h_feat_mat_te\n",
    "\n",
    "    # Apply sigmoid to get soft predictions\n",
    "    soft_predictions_te = 1 / (1 + np.exp(-a_class_te))\n",
    "\n",
    "    targets_te = targets_te.flatten() # reduces broadcasting issues\n",
    "    \n",
    "    # Calculate log-loss, using \n",
    "    loss_te = -np.mean(targets_te * np.log(soft_predictions_te) + (1 - targets_te) * np.log(1 - soft_predictions_te))\n",
    "    # And again for training data\n",
    "    \n",
    "    # calculate soft predictions:\n",
    "    \n",
    "    a_feat_mat_tr = learned_weights_feat @ inputs_tr.T # Produces a matrix M x N_te, where each column represents the a_feat of a datapoint \n",
    "    \n",
    "    # Apply ReLU\n",
    "    h_feat_mat_tr = np.maximum(a_feat_mat_tr, 0) \n",
    "\n",
    "    # Calculate a_class\n",
    "    a_class_tr = learned_weights_class @ h_feat_mat_tr\n",
    "\n",
    "    # Apply sigmoid to get soft predictions\n",
    "    soft_predictions_tr = 1 / (1 + np.exp(-a_class_tr))\n",
    "\n",
    "    targets_tr = targets_tr.flatten() # reduces broadcasting issues\n",
    "    \n",
    "    # Calculate log-loss\n",
    "    loss_tr = -np.mean(targets_tr * np.log(soft_predictions_tr) + (1 - targets_tr) * np.log(1 - soft_predictions_tr))\n",
    "\n",
    "\n",
    "    \n",
    "    return learned_weights_feat, learned_weights_class, loss_tr, loss_te\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a476b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned weights at the feature extraction layer (diabetes dataset):\n",
      "[[ 5.52106952e-01  1.07849692e+00 -1.27324764e+00 -2.91121730e-03\n",
      "  -2.81108249e-01 -5.01513908e-01 -5.57851746e-01]\n",
      " [ 2.76937262e-01  2.85540122e-01 -5.87688074e-01 -9.60109208e-02\n",
      "  -1.68971554e-01  6.57901532e-01 -4.78032165e-01]\n",
      " [ 5.06484828e-01 -4.13989679e-01 -3.29184281e-01  1.01355971e-01\n",
      "  -5.18132212e-01  3.30000000e-01 -4.40000000e-01]\n",
      " [-5.92020733e-01 -1.27989888e-01  6.97913719e-01 -3.59479901e-01\n",
      "   2.26461408e-01 -3.13281607e-01 -1.15085551e+00]\n",
      " [-1.02548424e+00  1.87189610e+00 -1.25581259e+00  9.86736115e-01\n",
      "  -5.78861276e-01  2.95570096e-01 -9.30926321e-01]\n",
      " [-2.53860921e+00  4.38748570e+00  8.48110186e-02 -3.88082713e-01\n",
      "  -1.77079470e+00  2.98590706e-01  3.03595976e-01]\n",
      " [ 7.30175759e-01 -3.52060525e-01 -5.15468970e-01 -5.49321435e-02\n",
      "   2.97705374e-01  7.08812032e-01  2.50833608e-01]\n",
      " [-3.90000000e-01  7.00000000e-02  8.00000000e-02 -5.70000000e-01\n",
      "  -2.00000000e-01 -3.50000000e-01 -4.30000000e-01]\n",
      " [ 8.82879426e-01 -1.82161866e+00 -5.09859265e-01  6.03224692e-01\n",
      "   2.10186505e+00  2.48697260e-01  1.28593966e-01]\n",
      " [ 3.74527805e+00 -2.08367650e+00 -1.25648396e+00 -2.94536461e-01\n",
      "  -8.24001098e-01 -1.93203350e-01 -3.84912794e-01]]\n",
      "Learned weights at the classification layer (diabetes dataset):\n",
      "[[-0.76234104  0.64186254  0.54925752  0.78577828 -1.13980613 -0.9368139\n",
      "   0.47593704  0.69        0.86081306 -3.05005949]]\n",
      "Train loss (diabetes dataset): 0.5834607474634426\n",
      "Estimated population loss for learned weights (diabetes dataset): 0.6051701519920735\n"
     ]
    }
   ],
   "source": [
    "# Init model parameters\n",
    "dataset_init_weights_feat = np.array([\n",
    "    [ 0.47,  0.13, -0.49,  0.25,  0.06, -0.08, -0.09],\n",
    "    [ 0.05,  0.03, -0.56,  0.13,  0.11,  0.33, -0.57],\n",
    "    [ 0.53, -0.4 , -0.32,  0.11, -0.51,  0.33, -0.44],\n",
    "    [-0.19,  0.18,  0.45, -0.58,  0.03,  0.31, -0.55],\n",
    "    [-0.33,  0.54, -0.2 ,  0.56,  0.07, -0.56,  0.53],\n",
    "    [ 0.37,  0.4 ,  0.03,  0.05,  0.4 ,  0.44,  0.58],\n",
    "    [ 0.5 ,  0.41, -0.18, -0.01,  0.19,  0.39, -0.32],\n",
    "    [-0.39,  0.07,  0.08, -0.57, -0.2 , -0.35, -0.43],\n",
    "    [-0.16, -0.05, -0.49, -0.51,  0.57,  0.35, -0.43],\n",
    "    [ 0.58, -0.48, -0.33, -0.43, -0.36, -0.11, -0.44]\n",
    "])\n",
    "dataset_init_weights_class = np.array([\n",
    "    [-0.2 ,  0.29,  0.55,  0.17,  0.3 , -0.19,  0.38,  0.69,  0.46, -0.52]\n",
    "])\n",
    "\n",
    "# Training parameters\n",
    "dataset_n_epochs = 50\n",
    "dataset_lr = 0.05\n",
    "\n",
    "\n",
    "# Train neural network and estimate the population loss for the learned weights\n",
    "dataset_learned_weights_feat, dataset_learned_weights_class, dataset_nn_loss_tr, dataset_nn_loss_te = train_and_test_neural_network(\n",
    "    np.copy(dataset_inputs_tr),\n",
    "    np.copy(dataset_targets_tr),\n",
    "    np.copy(dataset_inputs_te),\n",
    "    np.copy(dataset_targets_te),\n",
    "    dataset_init_weights_feat,\n",
    "    dataset_init_weights_class,\n",
    "    dataset_lr,\n",
    "    dataset_n_epochs\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Learned weights at the feature extraction layer (diabetes dataset):\\n{dataset_learned_weights_feat}\")\n",
    "print(f\"Learned weights at the classification layer (diabetes dataset):\\n{dataset_learned_weights_class}\")\n",
    "print(f\"Train loss (diabetes dataset): {dataset_nn_loss_tr}\")\n",
    "print(f\"Estimated population loss for learned weights (diabetes dataset): {dataset_nn_loss_te}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d088cfb8-58ea-403c-be3f-a05d37b15fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
